{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('Annomi_VGGish.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Type: <class 'NoneType'>\n",
      "Sample 9985 has a different type.\n",
      "Sample 5309 has a different type.\n",
      "Sample 7185 has a different type.\n",
      "Sample 12082 has a different type.\n",
      "Sample 10600 has a different type.\n"
     ]
    }
   ],
   "source": [
    "# Check the type of the embeddings\n",
    "embedding_type = type(df.iloc[0]['client_vggish_emb'])  # Assuming the embeddings column is named 'client_vggish_embeddings'\n",
    "print(\"Embedding Type:\", embedding_type)\n",
    "\n",
    "# Check the shape or length of a few random embeddings\n",
    "import random\n",
    "\n",
    "sample_indices = random.sample(range(len(df)), 5)\n",
    "for idx in sample_indices:\n",
    "    embedding_sample = df.iloc[idx]['client_vggish_emb']\n",
    "    \n",
    "    # Checking shape if it's a numpy array or length if it's a list\n",
    "    if isinstance(embedding_sample, list):\n",
    "        print(f\"Sample {idx} Embedding Length:\", len(embedding_sample))\n",
    "    elif isinstance(embedding_sample, np.ndarray):\n",
    "        print(f\"Sample {idx} Embedding Shape:\", embedding_sample.shape)\n",
    "    else:\n",
    "        print(f\"Sample {idx} has a different type.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two dataset where one interlocutor is client and the other is therapist\n",
    "client_df = df[df['interlocutor'] == 'client']\n",
    "therapist_df = df[df['interlocutor'] == 'therapist']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Determine the maximum sequence length for client embeddings\n",
    "# max_sequence_length_client = client_df['client_vggish_emb'].apply(lambda x: x.shape[0] if isinstance(x, np.ndarray) else 0).max()\n",
    "\n",
    "# # Padding function using PyTorch\n",
    "# def pad_sequences_torch(embeddings, max_len):\n",
    "#     padded_embeddings = torch.zeros((len(embeddings), max_len, 128))  # Assuming embeddings have 128 features\n",
    "#     for idx, emb in enumerate(embeddings):\n",
    "#         if isinstance(emb, np.ndarray):\n",
    "#             length = min(emb.shape[0], max_len)\n",
    "#             padded_embeddings[idx, :length] = torch.tensor(emb[:length])\n",
    "#     return padded_embeddings\n",
    "\n",
    "# # Pad client embeddings\n",
    "# padded_client_embeddings = pad_sequences_torch(client_df['client_vggish_emb'].tolist(), max_sequence_length_client)\n",
    "\n",
    "# print(max_sequence_length_client)\n",
    "# print(padded_client_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequences(sequences, padding_value=0):\n",
    "    \"\"\"\n",
    "    Pad the sequences to the same length with the given padding value.\n",
    "    \n",
    "    Args:\n",
    "    - sequences (list of torch.Tensor): List of sequences to be padded.\n",
    "    - padding_value (float): Value used for padding.\n",
    "    \n",
    "    Returns:\n",
    "    - torch.Tensor: Padded sequences.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the feature size from the first 2D tensor in the list\n",
    "    feature_size = next(seq.size(1) for seq in sequences if seq.dim() == 2)\n",
    "    \n",
    "    # Ensure all sequences have at least two dimensions with correct feature size\n",
    "    sequences = [seq.unsqueeze(-1).repeat(1, feature_size) if seq.dim() == 1 else seq for seq in sequences]\n",
    "    \n",
    "    # Get the maximum sequence length\n",
    "    max_len = max([seq.size(0) for seq in sequences])\n",
    "    \n",
    "    # Create a tensor for padded sequences\n",
    "    padded_seqs = torch.full((len(sequences), max_len, feature_size), padding_value)\n",
    "    \n",
    "    # Copy sequences to the padded tensor\n",
    "    for i, seq in enumerate(sequences):\n",
    "        padded_seqs[i, :seq.size(0)] = seq\n",
    "    \n",
    "    return padded_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert embeddings column to a list of tensors\n",
    "sequences = [torch.tensor(embedding) for embedding in client_df['client_vggish_emb']]\n",
    "padded_client_embeddings = pad_sequences(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = padded_client_embeddings\n",
    "y = client_df['client_talk_type'].values  # Adjust if the column name is different or if encoding is required\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):\n",
    "        super(SimpleLSTM, self).__init__()\n",
    "        \n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        \n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # LSTM layer\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        \n",
    "        # Only take the output from the final timestep\n",
    "        final_out = lstm_out[:, -1, :]\n",
    "        \n",
    "        # Fully connected layer\n",
    "        out = self.fc(final_out)\n",
    "        return out\n",
    "    \n",
    "class DeepLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, num_classes, dropout_prob=0.1):\n",
    "        super(DeepLSTM, self).__init__()\n",
    "        \n",
    "        # First LSTM layer\n",
    "        self.lstm1 = nn.LSTM(input_dim, hidden_dims[0], batch_first=True)\n",
    "        self.dropout1 = nn.Dropout(dropout_prob)\n",
    "        \n",
    "        # Second LSTM layer\n",
    "        self.lstm2 = nn.LSTM(hidden_dims[0], hidden_dims[1], batch_first=True)\n",
    "        self.dropout2 = nn.Dropout(dropout_prob)\n",
    "        \n",
    "        # Third LSTM layer\n",
    "        self.lstm3 = nn.LSTM(hidden_dims[1], hidden_dims[2], batch_first=True)\n",
    "        self.dropout3 = nn.Dropout(dropout_prob)\n",
    "        \n",
    "        # Fourth LSTM layer\n",
    "        self.lstm4 = nn.LSTM(hidden_dims[2], hidden_dims[3], batch_first=True)\n",
    "        self.dropout4 = nn.Dropout(dropout_prob)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(hidden_dims[3], 128)\n",
    "        self.dropout5 = nn.Dropout(dropout_prob)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.dropout6 = nn.Dropout(dropout_prob)\n",
    "        self.fc3 = nn.Linear(64, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # First LSTM\n",
    "        x, _ = self.lstm1(x)\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        # Second LSTM\n",
    "        x, _ = self.lstm2(x)\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        # Third LSTM\n",
    "        x, _ = self.lstm3(x)\n",
    "        x = self.dropout3(x)\n",
    "        \n",
    "        # Fourth LSTM (only keep the output of the last sequence)\n",
    "        x, _ = self.lstm4(x)\n",
    "        x = self.dropout4(x[:, -1, :])\n",
    "        \n",
    "        # Fully connected layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout5(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout6(x)\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        \n",
    "        self.lstm1 = nn.LSTM(input_dim, 256, batch_first=True)\n",
    "        self.dropout1 = nn.Dropout(0.1)\n",
    "        \n",
    "        self.lstm2 = nn.LSTM(256, 128, batch_first=True)\n",
    "        self.dropout2 = nn.Dropout(0.1)\n",
    "        \n",
    "        self.lstm3 = nn.LSTM(128, 64, batch_first=True)\n",
    "        self.dropout3 = nn.Dropout(0.1)\n",
    "        \n",
    "        self.lstm4 = nn.LSTM(64, 32, batch_first=True)\n",
    "        self.dropout4 = nn.Dropout(0.1)\n",
    "        \n",
    "        self.fc1 = nn.Linear(32, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, num_classes)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm1(x)\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        x, _ = self.lstm2(x)\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        x, _ = self.lstm3(x)\n",
    "        x = self.dropout3(x)\n",
    "        \n",
    "        x, _ = self.lstm4(x)\n",
    "        x = self.dropout4(x)\n",
    "        \n",
    "        x = x[:, -1, :]  # Use the last time step's output\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout4(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout4(x)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        x = self.softmax(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['change' 'neutral' 'sustain']\n"
     ]
    }
   ],
   "source": [
    "# Initialize and fit the encoder on y_train\n",
    "encoder = LabelEncoder()\n",
    "encoded_y_train = encoder.fit_transform(y_train)\n",
    "\n",
    "# Transform y_val using the same encoder\n",
    "encoded_y_val = encoder.transform(y_val)\n",
    "\n",
    "# Convert the encoded labels to PyTorch tensors\n",
    "y_train_tensor = torch.tensor(encoded_y_train)\n",
    "y_val_tensor = torch.tensor(encoded_y_val)\n",
    "\n",
    "# For verification, print out the unique classes the encoder recognized\n",
    "print(encoder.classes_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert X_train, X_val, encoded_y_train, and encoded_y_val to tensors if they aren't already\n",
    "if not isinstance(X_train, torch.Tensor):\n",
    "    X_train_tensor = torch.stack([torch.Tensor(e) for e in X_train])\n",
    "else:\n",
    "    X_train_tensor = X_train\n",
    "\n",
    "if not isinstance(X_val, torch.Tensor):\n",
    "    X_val_tensor = torch.stack([torch.Tensor(e) for e in X_val])\n",
    "else:\n",
    "    X_val_tensor = X_val\n",
    "\n",
    "y_train_tensor = torch.tensor(encoded_y_train, dtype=torch.long) if not isinstance(encoded_y_train, torch.Tensor) else encoded_y_train\n",
    "y_val_tensor = torch.tensor(encoded_y_val, dtype=torch.long) if not isinstance(encoded_y_val, torch.Tensor) else encoded_y_val\n",
    "\n",
    "# Create TensorDatasets and DataLoaders\n",
    "train_data = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_data = TensorDataset(X_val_tensor, y_val_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "val_loader = DataLoader(val_data, shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Hyperparameters\n",
    "# input_dim = 128\n",
    "# hidden_dim = 64\n",
    "# output_dim = len(encoder.classes_)  # Number of unique classes in the target label\n",
    "# num_layers = 2\n",
    "\n",
    "# model = SimpleLSTM(input_dim, hidden_dim, output_dim, num_layers).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Hyperparameters\n",
    "# input_dim = 128  # Embedding size\n",
    "# hidden_dims = [256, 128, 64, 32]\n",
    "# num_classes = len(encoder.classes_)  # Number of unique classes\n",
    "\n",
    "# model = DeepLSTM(input_dim, hidden_dims, num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMModel(input_dim=128, num_classes=3).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Train Loss: 0.9328\n",
      "Validation Loss: 0.9268, F1 Macro: 0.2563\n",
      "Epoch 2/100, Train Loss: 0.9269\n",
      "Validation Loss: 0.9268, F1 Macro: 0.2563\n",
      "Epoch 3/100, Train Loss: 0.9269\n",
      "Validation Loss: 0.9268, F1 Macro: 0.2563\n",
      "Epoch 4/100, Train Loss: 0.9269\n",
      "Validation Loss: 0.9268, F1 Macro: 0.2563\n",
      "Epoch 5/100, Train Loss: 0.9269\n",
      "Validation Loss: 0.9268, F1 Macro: 0.2563\n",
      "Epoch 6/100, Train Loss: 0.9268\n",
      "Validation Loss: 0.9268, F1 Macro: 0.2563\n",
      "Epoch 7/100, Train Loss: 0.9268\n",
      "Validation Loss: 0.9268, F1 Macro: 0.2563\n",
      "Epoch 8/100, Train Loss: 0.9268\n",
      "Validation Loss: 0.9268, F1 Macro: 0.2563\n",
      "Epoch 9/100, Train Loss: 0.9268\n",
      "Validation Loss: 0.9268, F1 Macro: 0.2563\n",
      "Epoch 10/100, Train Loss: 0.9268\n",
      "Validation Loss: 0.9268, F1 Macro: 0.2563\n",
      "Epoch 11/100, Train Loss: 0.9268\n",
      "Validation Loss: 0.9268, F1 Macro: 0.2563\n",
      "Epoch 12/100, Train Loss: 0.9268\n",
      "Validation Loss: 0.9268, F1 Macro: 0.2563\n",
      "Epoch 13/100, Train Loss: 0.9268\n",
      "Validation Loss: 0.9268, F1 Macro: 0.2563\n",
      "Epoch 14/100, Train Loss: 0.9268\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/mnt/workplace/annomi_classification/mtl_audio.ipynb Cell 17\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a226e76696469615f7562756e74755f637564615f31312e38222c2273657474696e6773223a7b22686f7374223a227373683a2f2f6b6174656461616e2e73656d6c612e64666b692e646576227d7d/mnt/workplace/annomi_classification/mtl_audio.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=35'>36</a>\u001b[0m all_labels \u001b[39m=\u001b[39m []\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a226e76696469615f7562756e74755f637564615f31312e38222c2273657474696e6773223a7b22686f7374223a227373683a2f2f6b6174656461616e2e73656d6c612e64666b692e646576227d7d/mnt/workplace/annomi_classification/mtl_audio.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a226e76696469615f7562756e74755f637564615f31312e38222c2273657474696e6773223a7b22686f7374223a227373683a2f2f6b6174656461616e2e73656d6c612e64666b692e646576227d7d/mnt/workplace/annomi_classification/mtl_audio.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=38'>39</a>\u001b[0m     \u001b[39mfor\u001b[39;00m inputs, labels \u001b[39min\u001b[39;00m val_loader:\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a226e76696469615f7562756e74755f637564615f31312e38222c2273657474696e6773223a7b22686f7374223a227373683a2f2f6b6174656461616e2e73656d6c612e64666b692e646576227d7d/mnt/workplace/annomi_classification/mtl_audio.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=39'>40</a>\u001b[0m         inputs, labels \u001b[39m=\u001b[39m inputs\u001b[39m.\u001b[39mto(device), labels\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a226e76696469615f7562756e74755f637564615f31312e38222c2273657474696e6773223a7b22686f7374223a227373683a2f2f6b6174656461616e2e73656d6c612e64666b692e646576227d7d/mnt/workplace/annomi_classification/mtl_audio.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=41'>42</a>\u001b[0m         outputs \u001b[39m=\u001b[39m model(inputs\u001b[39m.\u001b[39mfloat())\n",
      "File \u001b[0;32m~/miniconda3/envs/mlStuff/lib/python3.9/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/mlStuff/lib/python3.9/site-packages/torch/utils/data/dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    676\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 677\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    679\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/mlStuff/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
      "File \u001b[0;32m~/miniconda3/envs/mlStuff/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:265\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdefault_collate\u001b[39m(batch):\n\u001b[1;32m    205\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[39m        Function that takes in a batch of data and puts the elements within the batch\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[39m        into a tensor with an additional outer dimension - batch size. The exact output type can be\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[39m            >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 265\u001b[0m     \u001b[39mreturn\u001b[39;00m collate(batch, collate_fn_map\u001b[39m=\u001b[39;49mdefault_collate_fn_map)\n",
      "File \u001b[0;32m~/miniconda3/envs/mlStuff/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:142\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    139\u001b[0m transposed \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch))  \u001b[39m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m--> 142\u001b[0m     \u001b[39mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[39m=\u001b[39mcollate_fn_map) \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m transposed]  \u001b[39m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/mlStuff/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:142\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    139\u001b[0m transposed \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch))  \u001b[39m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m--> 142\u001b[0m     \u001b[39mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[39m=\u001b[39;49mcollate_fn_map) \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m transposed]  \u001b[39m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/mlStuff/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:119\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[39mif\u001b[39;00m collate_fn_map \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    118\u001b[0m     \u001b[39mif\u001b[39;00m elem_type \u001b[39min\u001b[39;00m collate_fn_map:\n\u001b[0;32m--> 119\u001b[0m         \u001b[39mreturn\u001b[39;00m collate_fn_map[elem_type](batch, collate_fn_map\u001b[39m=\u001b[39;49mcollate_fn_map)\n\u001b[1;32m    121\u001b[0m     \u001b[39mfor\u001b[39;00m collate_type \u001b[39min\u001b[39;00m collate_fn_map:\n\u001b[1;32m    122\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[0;32m~/miniconda3/envs/mlStuff/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:162\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    160\u001b[0m     storage \u001b[39m=\u001b[39m elem\u001b[39m.\u001b[39m_typed_storage()\u001b[39m.\u001b[39m_new_shared(numel, device\u001b[39m=\u001b[39melem\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    161\u001b[0m     out \u001b[39m=\u001b[39m elem\u001b[39m.\u001b[39mnew(storage)\u001b[39m.\u001b[39mresize_(\u001b[39mlen\u001b[39m(batch), \u001b[39m*\u001b[39m\u001b[39mlist\u001b[39m(elem\u001b[39m.\u001b[39msize()))\n\u001b[0;32m--> 162\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mstack(batch, \u001b[39m0\u001b[39;49m, out\u001b[39m=\u001b[39;49mout)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "best_f1 = 0  # For tracking the best F1 score\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs.float())\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "    \n",
    "    # Compute average training loss for the epoch\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {epoch_loss:.4f}\")\n",
    "    \n",
    "    # Evaluate the model on the validation set\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs.float())\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "            \n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Compute average validation loss and F1 score for the epoch\n",
    "    val_loss = val_loss / len(val_loader.dataset)\n",
    "    f1_macro = f1_score(all_labels, all_preds, average='macro')\n",
    "    print(f\"Validation Loss: {val_loss:.4f}, F1 Macro: {f1_macro:.4f}\")\n",
    "    \n",
    "    # Save model with best F1 score\n",
    "    if f1_macro > best_f1:\n",
    "        best_f1 = f1_macro\n",
    "        torch.save(model.state_dict(), 'best_model_audio_client_vggish.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load the best model\n",
    "model.load_state_dict(torch.load('best_model_audio_client_vggish.pth'))\n",
    "\n",
    "# Get predictions\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in val_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        outputs = model(inputs.float())\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(all_labels, all_preds, target_names=encoder.classes_))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlStuff",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
