{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoTokenizer, Trainer, TrainingArguments\n",
    "import torch\n",
    "from transformers import RobertaConfig, RobertaModel\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import f1_score\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "import torch.nn.functional as F\n",
    "from transformers import Wav2Vec2PreTrainedModel, Wav2Vec2Model\n",
    "from transformers import TrainerCallback, TrainerState, TrainerControl, AutoModelForAudioClassification\n",
    "from transformers import DataCollator\n",
    "from transformers import EvalPrediction\n",
    "from torch import optim\n",
    "from transformers import AutoFeatureExtractor\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_audio = pd.read_pickle('AnnoMI-ast-new.pkl')\n",
    "df_text = pd.read_pickle('AnnoMI-full-with-audio-cleaned-text.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a dataset where intelocutor is client\n",
    "df_client_audio = df_audio[df_audio['interlocutor'] == 'client']\n",
    "df_client_audio = df_client_audio[['client_ast_emb', 'client_talk_type']]\n",
    "df_client_audio.rename(columns={'client_ast_emb': 'inputs', 'client_talk_type': 'labels'}, inplace=True)\n",
    "df_client_audio['labels'] = df_client_audio['labels'].astype(\"category\").cat.codes\n",
    "\n",
    "df_therapist_audio = df_audio[df_audio['interlocutor'] == 'therapist']\n",
    "df_therapist_audio = df_therapist_audio[['therapist_ast_emb', 'main_therapist_behaviour']]\n",
    "df_therapist_audio.rename(columns={'therapist_ast_emb': 'inputs', 'main_therapist_behaviour': 'labels'}, inplace=True)\n",
    "df_therapist_audio['labels'] = df_therapist_audio['labels'].astype(\"category\").cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe):  # Add label2id as an argument\n",
    "        self.data = dataframe\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label = torch.tensor(self.data.iloc[idx]['labels'], dtype=torch.long)\n",
    "        \n",
    "        input_values = torch.tensor(self.data.iloc[idx]['inputs'], dtype=torch.float).squeeze(0)\n",
    "        \n",
    "        return {\n",
    "            \"input_values\": input_values,\n",
    "            \"labels\": label  # Use the encoded label\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_dataset_audio = CustomDataset(df_client_audio)\n",
    "therapist_dataset_audio = CustomDataset(df_therapist_audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_dataloader_audio = DataLoader(client_dataset_audio, batch_size=8, shuffle=False)\n",
    "therapist_dataloader_audio = DataLoader(therapist_dataset_audio, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MTLASTAudioClassificaiton(nn.Module):\n",
    "    def __init__(self, base_model_name, num_classes_client, num_classes_therapist):\n",
    "        super(MTLASTAudioClassificaiton, self).__init__()\n",
    "\n",
    "        # Shared layer\n",
    "        self.base_model = AutoModelForAudioClassification.from_pretrained(base_model_name)\n",
    "\n",
    "        # Client specific classifier\n",
    "        self.client_classifier = nn.Linear(527, num_classes_client)  # Adjust the input dimension\n",
    "\n",
    "        # Therapist specific classifier\n",
    "        self.therapist_classifier = nn.Linear(527, num_classes_therapist)  # Adjust the input dimension\n",
    "\n",
    "    def forward(self, input_values, task_name=None, return_embeddings=False):\n",
    "        # Passing input_values through the shared layer\n",
    "        shared_output = self.base_model(input_values=input_values).logits\n",
    "        \n",
    "        # If return_embeddings is True, return the shared_output directly\n",
    "        if return_embeddings:\n",
    "            return shared_output\n",
    "        \n",
    "        # pooled_output = torch.mean(shared_output, dim=1)  # Only if you want mean pooling\n",
    "        pooled_output = shared_output  # Use directly if not pooling\n",
    "\n",
    "        # Routing through the appropriate classifier\n",
    "        if task_name == 'client':\n",
    "            return self.client_classifier(pooled_output)\n",
    "        elif task_name == 'therapist':\n",
    "            return self.therapist_classifier(pooled_output)\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid task_name: {task_name}. Expected 'client' or 'therapist'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "num_classes_client = 3\n",
    "num_classes_therapist = 4\n",
    "base_model_name = \"MIT/ast-finetuned-audioset-10-10-0.4593\"  # This is an example name; adjust as needed\n",
    "\n",
    "model_client_audio = MTLASTAudioClassificaiton(base_model_name, num_classes_client, num_classes_therapist).to(device)\n",
    "\n",
    "# Load the saved weights (for demonstration, I'm using the client weights as an example)\n",
    "model_path_client_audio = \"best_mtl_model_audio_ast_client.pth\"  # Update the path accordingly\n",
    "model_client_audio.load_state_dict(torch.load(model_path_client_audio))\n",
    "\n",
    "\n",
    "model_therapist_audio = MTLASTAudioClassificaiton(base_model_name, num_classes_client, num_classes_therapist).to(device)\n",
    "model_path_therapist_audio = \"best_mtl_model_audio_ast_therapist.pth\"\n",
    "model_therapist_audio.load_state_dict(torch.load(model_path_therapist_audio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_audio_embeddings_and_labels_from_dataloader(model, dataloader):\n",
    "    all_embeddings = []\n",
    "    all_labels = []\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    progress_bar = tqdm(dataloader, desc=\"Extracting embeddings and labels\")\n",
    "    with torch.no_grad():\n",
    "        for batch in progress_bar:\n",
    "            input_values = batch[\"input_values\"].to(device)\n",
    "            embeddings = model(input_values=input_values, return_embeddings=True)\n",
    "            del input_values\n",
    "            all_embeddings.append(embeddings)\n",
    "            all_labels.append(batch[\"labels\"])\n",
    "            del embeddings\n",
    "    torch.cuda.empty_cache()\n",
    "    return torch.cat(all_embeddings, dim=0), torch.cat(all_labels, dim=0)  # Concatenate embeddings and labels along the batch dimension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_embeddings_client, audio_labels_client = extract_audio_embeddings_and_labels_from_dataloader(model_client_audio, client_dataloader_audio)\n",
    "audio_embeddings_therapist, audio_labels_therapist = extract_audio_embeddings_and_labels_from_dataloader(model_therapist_audio, therapist_dataloader_audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the client audio embeddings and labels\n",
    "import pickle\n",
    "\n",
    "with open('audio_embeddings_ast_client.pkl', 'wb') as file:\n",
    "    pickle.dump({\n",
    "        \"embeddings\": audio_embeddings_client.cpu().numpy(),\n",
    "        \"labels\": audio_labels_client.cpu().numpy()\n",
    "    }, file)\n",
    "\n",
    "# Save the therapist audio embeddings and labels\n",
    "with open('audio_embeddings_ast_therapist.pkl', 'wb') as file:\n",
    "    pickle.dump({\n",
    "        \"embeddings\": audio_embeddings_therapist.cpu().numpy(),\n",
    "        \"labels\": audio_labels_therapist.cpu().numpy()\n",
    "    }, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free up GPU memory\n",
    "del model_client_audio, model_therapist_audio, audio_embeddings_client, audio_labels_client, audio_embeddings_therapist, audio_labels_therapist\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_roberta_large = AutoTokenizer.from_pretrained(\"roberta-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer\n",
    "tokenizer = tokenizer_roberta_large\n",
    "\n",
    "# Tokenize the utterances for both tasks\n",
    "def tokenize_data(texts):\n",
    "    return tokenizer(texts.tolist(), truncation=True, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "# Tokenize the client utterances\n",
    "client_texts = df_text[df_text['interlocutor'] == 'client']['utterance_text']\n",
    "client_labels = df_text[df_text['interlocutor'] == 'client']['client_talk_type'].astype(\"category\").cat.codes\n",
    "client_encodings = tokenize_data(client_texts)\n",
    "\n",
    "# Tokenize the therapist utterances\n",
    "therapist_texts = df_text[df_text['interlocutor'] == 'therapist']['utterance_text']\n",
    "therapist_labels = df_text[df_text['interlocutor'] == 'therapist']['main_therapist_behaviour'].astype(\"category\").cat.codes\n",
    "therapist_encodings = tokenize_data(therapist_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch Dataset\n",
    "class MTLDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx].clone().detach() for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_dataset = MTLDataset(client_encodings, client_labels.to_numpy())\n",
    "therapist_dataset = MTLDataset(therapist_encodings, therapist_labels.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MTLModel(nn.Module):\n",
    "    def __init__(self, base_model_name, num_classes_client, num_classes_therapist):\n",
    "        super(MTLModel, self).__init__()\n",
    "        \n",
    "        # Shared layers using Roberta\n",
    "        self.shared = RobertaModel.from_pretrained(base_model_name)\n",
    "        \n",
    "        # Task-specific heads\n",
    "        self.client_classifier = nn.Linear(self.shared.config.hidden_size, num_classes_client)\n",
    "        self.therapist_classifier = nn.Linear(self.shared.config.hidden_size, num_classes_therapist)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, task_name=None, return_embeddings=False):\n",
    "        shared_output = self.shared(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = shared_output[0][:, 0, :]\n",
    "        \n",
    "        # If return_embeddings is True, return the pooled_output directly\n",
    "        if return_embeddings:\n",
    "            return pooled_output\n",
    "        \n",
    "        # Route through the appropriate classifier\n",
    "        if task_name == \"client\":\n",
    "            return self.client_classifier(pooled_output)\n",
    "        elif task_name == \"therapist\":\n",
    "            return self.therapist_classifier(pooled_output)\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid task_name: {task_name}. Expected 'client' or 'therapist'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of unique labels for each task\n",
    "num_classes_client = 3\n",
    "num_classes_therapist = 4\n",
    "\n",
    "# Initialize the model for client task\n",
    "base_model_name = \"roberta-large\"\n",
    "model_client_text = MTLModel(base_model_name, num_classes_client, num_classes_therapist).to(device)\n",
    "\n",
    "# Load the saved weights\n",
    "model_path_client_text = \"best_mtl_model_roberta_client.pth\"\n",
    "model_client_text.load_state_dict(torch.load(model_path_client_text))\n",
    "\n",
    "\n",
    "# Initialize the model for therapist task\n",
    "model_therapist_text = MTLModel(base_model_name, num_classes_client, num_classes_therapist).to(device)\n",
    "\n",
    "# Load the saved weights\n",
    "model_path_therapist_text = \"best_mtl_model_roberta_therapist.pth\"\n",
    "model_therapist_text.load_state_dict(torch.load(model_path_therapist_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loaders\n",
    "batch_size = 32\n",
    "\n",
    "client_dataloader_text = DataLoader(client_dataset, batch_size=batch_size, shuffle=False)\n",
    "therapist_dataloader_text = DataLoader(therapist_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_embeddings_and_labels_from_dataloader(model, dataloader):\n",
    "    all_embeddings = []\n",
    "    all_labels = []\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    progress_bar = tqdm(dataloader, desc=\"Extracting text embeddings and labels\")\n",
    "    with torch.no_grad():\n",
    "        for batch in progress_bar:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            embeddings = model(input_ids=input_ids, attention_mask=attention_mask, return_embeddings=True)\n",
    "            del input_ids, attention_mask\n",
    "            all_embeddings.append(embeddings)\n",
    "            all_labels.append(batch[\"labels\"])\n",
    "            del embeddings\n",
    "    torch.cuda.empty_cache()\n",
    "    return torch.cat(all_embeddings, dim=0), torch.cat(all_labels, dim=0)  # Concatenate embeddings and labels along the batch dimension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_embeddings_client, text_labels_client = extract_text_embeddings_and_labels_from_dataloader(model_client_text, client_dataloader_text)\n",
    "text_embeddings_therapist, text_labels_therapist = extract_text_embeddings_and_labels_from_dataloader(model_therapist_text, therapist_dataloader_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the client text embeddings and labels\n",
    "with open('text_embeddings_roberta_client.pkl', 'wb') as file:\n",
    "    pickle.dump({\n",
    "        \"embeddings\": text_embeddings_client.cpu().numpy(),\n",
    "        \"labels\": text_labels_client.cpu().numpy()\n",
    "    }, file)\n",
    "\n",
    "# Save the therapist text embeddings and labels\n",
    "with open('text_embeddings_roberta_therapist.pkl', 'wb') as file:\n",
    "    pickle.dump({\n",
    "        \"embeddings\": text_embeddings_therapist.cpu().numpy(),\n",
    "        \"labels\": text_labels_therapist.cpu().numpy()\n",
    "    }, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free up GPU memory\n",
    "del model_client_text, model_therapist_text, text_embeddings_client, text_labels_client, text_embeddings_therapist, text_labels_therapist\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
