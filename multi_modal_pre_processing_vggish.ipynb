{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-12T05:21:23.379820Z",
     "iopub.status.busy": "2023-09-12T05:21:23.379353Z",
     "iopub.status.idle": "2023-09-12T05:21:28.049728Z",
     "shell.execute_reply": "2023-09-12T05:21:28.048652Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import torch\n",
    "\n",
    "df = pd.read_pickle('AnnoMI-full-with-audio-cleaned-text.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-12T05:21:28.052570Z",
     "iopub.status.busy": "2023-09-12T05:21:28.052303Z",
     "iopub.status.idle": "2023-09-12T05:21:28.077371Z",
     "shell.execute_reply": "2023-09-12T05:21:28.076344Z"
    }
   },
   "outputs": [],
   "source": [
    "# Check if cude is available then set device accordingly\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-12T05:21:28.080235Z",
     "iopub.status.busy": "2023-09-12T05:21:28.079894Z",
     "iopub.status.idle": "2023-09-12T05:21:28.288063Z",
     "shell.execute_reply": "2023-09-12T05:21:28.287111Z"
    }
   },
   "outputs": [],
   "source": [
    "import soundfile as sf\n",
    "import numpy as np\n",
    "import os\n",
    "from torchvggish import vggish, vggish_input\n",
    "import h5py\n",
    "import wavio\n",
    "from pydub import AudioSegment\n",
    "import resampy\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-12T05:21:28.291052Z",
     "iopub.status.busy": "2023-09-12T05:21:28.290504Z",
     "iopub.status.idle": "2023-09-12T05:21:28.293679Z",
     "shell.execute_reply": "2023-09-12T05:21:28.293054Z"
    }
   },
   "outputs": [],
   "source": [
    "# audio_list =  test['therapist_audio_utterance'].values[28]\n",
    "# audio_list = audio_list.tolist()\n",
    "# sf.write('test.wav', audio_list, 44100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-12T05:21:28.296053Z",
     "iopub.status.busy": "2023-09-12T05:21:28.295601Z",
     "iopub.status.idle": "2023-09-12T05:21:28.298395Z",
     "shell.execute_reply": "2023-09-12T05:21:28.297774Z"
    }
   },
   "outputs": [],
   "source": [
    "# audio_list = df['client_audio_utterance'][11647] \n",
    "# audio_list = audio_list.tolist()\n",
    "# audio_list += df['therapist_audio_utterance'][11648].tolist()\n",
    "\n",
    "# sf.write('current.wav', audio_list, 44100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-12T05:21:28.300844Z",
     "iopub.status.busy": "2023-09-12T05:21:28.300223Z",
     "iopub.status.idle": "2023-09-12T05:21:28.303153Z",
     "shell.execute_reply": "2023-09-12T05:21:28.302550Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Initialise model and download weights\n",
    "# embedding_model = vggish()\n",
    "# embedding_model.eval()\n",
    "\n",
    "\n",
    "# example = vggish_input.wavfile_to_examples('test.wav')\n",
    "# embeddings = embedding_model.forward(example)\n",
    "\n",
    "# embeddings = embeddings.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-12T05:21:28.305654Z",
     "iopub.status.busy": "2023-09-12T05:21:28.305064Z",
     "iopub.status.idle": "2023-09-12T05:21:28.791780Z",
     "shell.execute_reply": "2023-09-12T05:21:28.790747Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create two new columns for embeddings and initialize with None\n",
    "df['client_vggish_emb'] = None\n",
    "df['therapist_vggish_emb'] = None\n",
    "\n",
    "# Initialise the VGGish model\n",
    "embedding_model = vggish()\n",
    "embedding_model.eval()\n",
    "\n",
    "def resample_audio(filename, target_sr):\n",
    "    y, sr = librosa.load(filename, sr=None)  # Load with the original sample rate\n",
    "    y_resampled = resampy.resample(y, sr, target_sr)  # Resample to target sample rate\n",
    "    sf.write(filename, y_resampled, target_sr)\n",
    "\n",
    "# Function to compute embeddings for a given role\n",
    "def compute_embeddings(role, audio_list):\n",
    "    if audio_list is None:\n",
    "        return None\n",
    "    if len(audio_list) == 0:\n",
    "        # Padding the audio\n",
    "        pad_ms = 1000  # milliseconds of silence needed\n",
    "        silence = AudioSegment.silent(duration=pad_ms)\n",
    "\n",
    "        padded = silence\n",
    "        padded.export('current.wav', format='wav')\n",
    "        resample_audio('current.wav', 16000)\n",
    "\n",
    "        # Compute the embeddings again\n",
    "        example = vggish_input.wavfile_to_examples('current.wav')\n",
    "        embeddings = embedding_model.forward(example)\n",
    "        return embeddings.detach().numpy()\n",
    "\n",
    "    audio_list = audio_list.tolist()\n",
    "\n",
    "    # Save the audio data\n",
    "    sf.write('current.wav', audio_list, 44100)\n",
    "    resample_audio('current.wav', 16000)\n",
    "\n",
    "    try:\n",
    "        # Compute the embeddings\n",
    "        example = vggish_input.wavfile_to_examples('current.wav')\n",
    "        embeddings = embedding_model.forward(example)\n",
    "        return embeddings.detach().numpy()\n",
    "    except RuntimeError:\n",
    "        # Padding the audio\n",
    "        pad_ms = 1000  # milliseconds of silence needed\n",
    "        silence = AudioSegment.silent(duration=pad_ms)\n",
    "        audio = AudioSegment.from_wav('current.wav')\n",
    "\n",
    "        padded = audio + silence  # Adding silence after the audio\n",
    "        padded.export('current.wav', format='wav')\n",
    "        resample_audio('current.wav', 16000)\n",
    "\n",
    "        # Compute the embeddings again\n",
    "        example = vggish_input.wavfile_to_examples('current.wav')\n",
    "        embeddings = embedding_model.forward(example)\n",
    "        return embeddings.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-12T05:21:28.796075Z",
     "iopub.status.busy": "2023-09-12T05:21:28.795590Z",
     "iopub.status.idle": "2023-09-12T06:22:39.199401Z",
     "shell.execute_reply": "2023-09-12T06:22:39.198697Z"
    }
   },
   "outputs": [],
   "source": [
    "df_vggish = pd.DataFrame()\n",
    "\n",
    "# Iterate through the DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    # Determine the role (client or therapist) from the interlocutor column\n",
    "    role = row['interlocutor']\n",
    "    video_title = row['video_title']\n",
    "    utterance = row['utterance_id']\n",
    "    \n",
    "    print(f'Processing {role} {utterance} {video_title}...')\n",
    "\n",
    "    audio_list = row[f'{role}_audio_utterance']\n",
    "\n",
    "    # Compute the embeddings for the given role\n",
    "    embeddings = compute_embeddings(role, audio_list)\n",
    "\n",
    "    # Save the embeddings in the DataFrame\n",
    "    df.at[index, f'{role}_vggish_emb'] = embeddings\n",
    "    df_vggish = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-12T06:22:39.202205Z",
     "iopub.status.busy": "2023-09-12T06:22:39.201893Z",
     "iopub.status.idle": "2023-09-12T06:22:49.340791Z",
     "shell.execute_reply": "2023-09-12T06:22:49.340031Z"
    }
   },
   "outputs": [],
   "source": [
    "df.to_pickle('Annomi_VGGish.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlStuff",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
