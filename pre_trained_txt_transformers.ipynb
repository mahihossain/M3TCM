{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-12T15:39:43.923180Z",
     "iopub.status.busy": "2023-09-12T15:39:43.922596Z",
     "iopub.status.idle": "2023-09-12T15:39:46.819873Z",
     "shell.execute_reply": "2023-09-12T15:39:46.818858Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn import preprocessing\n",
    "from transformers import BertTokenizer, BertModel, BertPreTrainedModel, TrainingArguments, Trainer\n",
    "from transformers import RobertaTokenizer, RobertaPreTrainedModel, RobertaModel, AutoTokenizer, AutoModel, PreTrainedModel\n",
    "from transformers import TrainerCallback\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from transformers import TrainingArguments, AutoModel\n",
    "from transformers import TrainerCallback, TrainerState, TrainerControl\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-12T15:39:46.823721Z",
     "iopub.status.busy": "2023-09-12T15:39:46.822895Z",
     "iopub.status.idle": "2023-09-12T15:39:46.883042Z",
     "shell.execute_reply": "2023-09-12T15:39:46.881994Z"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-12T15:39:46.885868Z",
     "iopub.status.busy": "2023-09-12T15:39:46.885368Z",
     "iopub.status.idle": "2023-09-12T15:39:50.785981Z",
     "shell.execute_reply": "2023-09-12T15:39:50.784959Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_pickle('AnnoMI-full-with-audio-cleaned-text.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-12T15:39:50.789817Z",
     "iopub.status.busy": "2023-09-12T15:39:50.789421Z",
     "iopub.status.idle": "2023-09-12T15:39:50.812462Z",
     "shell.execute_reply": "2023-09-12T15:39:50.811744Z"
    }
   },
   "outputs": [],
   "source": [
    "df_client = df[df['interlocutor'] == 'client']\n",
    "#df_therapist = df[df['interlocutor'] == 'therapist']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-12T15:39:50.843191Z",
     "iopub.status.busy": "2023-09-12T15:39:50.842653Z",
     "iopub.status.idle": "2023-09-12T15:39:50.849259Z",
     "shell.execute_reply": "2023-09-12T15:39:50.848613Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original classes ['change' 'neutral' 'sustain']\n",
      "Corresponding numeric classes [0 1 2]\n",
      "X: (6338,)\n",
      "y: (6338,) [0 1 2]\n"
     ]
    }
   ],
   "source": [
    "X = df_client['utterance_text']\n",
    "y_text = df_client['client_talk_type']\n",
    "\n",
    "# X = df_therapist['utterance_text']\n",
    "# y_text = df_therapist['main_therapist_behaviour']\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(y_text)\n",
    "print(f'Original classes {le.classes_}')\n",
    "print(f'Corresponding numeric classes {le.transform(le.classes_)}')\n",
    "y =le.transform(y_text)\n",
    "print(f\"X: {X.shape}\")\n",
    "print(f\"y: {y.shape} {np.unique(y)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-12T15:39:50.851662Z",
     "iopub.status.busy": "2023-09-12T15:39:50.851206Z",
     "iopub.status.idle": "2023-09-12T15:39:50.855348Z",
     "shell.execute_reply": "2023-09-12T15:39:50.854728Z"
    }
   },
   "outputs": [],
   "source": [
    "# Splitting\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-12T15:39:50.857674Z",
     "iopub.status.busy": "2023-09-12T15:39:50.857223Z",
     "iopub.status.idle": "2023-09-12T15:39:51.860001Z",
     "shell.execute_reply": "2023-09-12T15:39:51.858853Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer_twitter_sentiment = AutoTokenizer.from_pretrained('cardiffnlp/twitter-roberta-base-sentiment-latest')\n",
    "tokenizer_distilbert = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "tokenizer_goemotion = AutoTokenizer.from_pretrained('SamLowe/roberta-base-go_emotions')\n",
    "tokenizer_roberta_large = AutoTokenizer.from_pretrained('roberta-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-12T15:39:51.862894Z",
     "iopub.status.busy": "2023-09-12T15:39:51.862465Z",
     "iopub.status.idle": "2023-09-12T15:39:52.410190Z",
     "shell.execute_reply": "2023-09-12T15:39:52.409086Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = tokenizer_goemotion\n",
    "train_encodings = tokenizer(train_texts.tolist(), truncation=True, padding=True)\n",
    "val_encodings = tokenizer(val_texts.tolist(), truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-12T15:39:52.414409Z",
     "iopub.status.busy": "2023-09-12T15:39:52.413544Z",
     "iopub.status.idle": "2023-09-12T15:39:52.435352Z",
     "shell.execute_reply": "2023-09-12T15:39:52.434710Z"
    }
   },
   "outputs": [],
   "source": [
    "class ClassificationDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = np.array(labels).astype('int')\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "class StopOnZeroLossCallback(TrainerCallback):\n",
    "    def on_log(self, args, state: TrainerState, control: TrainerControl, logs=None, **kwargs):\n",
    "        # Check if the training loss is exactly zero\n",
    "        if logs.get(\"loss\", 1) == 0: \n",
    "            print(\"Training loss reached zero, stopping training!\")\n",
    "            control.should_training_stop = True\n",
    "    \n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        log_prob = F.log_softmax(inputs, dim=-1)\n",
    "        prob = torch.exp(log_prob)\n",
    "        return F.nll_loss(\n",
    "            ((1 - prob) ** self.gamma) * log_prob,\n",
    "            targets,\n",
    "            reduction=self.reduction\n",
    "        )\n",
    "    \n",
    "# Function to compute f1_macro\n",
    "def f1_macro(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=1)\n",
    "    return {'f1_macro': f1_score(labels, predictions, average='macro')}\n",
    "\n",
    "class ThresholdEarlyStoppingCallback(TrainerCallback):\n",
    "    def on_evaluate(self, args, state, control, metrics, **kwargs):\n",
    "        f1 = metrics['eval_f1_macro']\n",
    "        if f1 > 0.6:\n",
    "            control.should_training_stop = True\n",
    "        return control\n",
    "    \n",
    "# A custom class that would stop the training if the validation loss rises for 5 consecutive epochs\n",
    "class StopOnRisingValidationLossCallback(TrainerCallback):\n",
    "    def __init__(self):\n",
    "        self.counter = 0\n",
    "        self.best_loss = np.inf\n",
    "        \n",
    "    def on_evaluate(self, args, state, control, metrics, **kwargs):\n",
    "        val_loss = metrics['eval_loss']\n",
    "        if val_loss < self.best_loss:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= 5:\n",
    "                control.should_training_stop = True\n",
    "        return control\n",
    "# A custom class that would stop the training if the validation f1_macro go down for 5 consecutive epochs\n",
    "class StopOnF1MacroCallback(TrainerCallback):\n",
    "    def __init__(self):\n",
    "        self.counter = 0\n",
    "        self.best_f1 = 0\n",
    "        \n",
    "    def on_evaluate(self, args, state, control, metrics, **kwargs):\n",
    "        f1 = metrics['eval_f1_macro']\n",
    "        if f1 > self.best_f1:\n",
    "            self.best_f1 = f1\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= 5:\n",
    "                control.should_training_stop = True\n",
    "        return control\n",
    "\n",
    "class RobertaClassificationTwitter(nn.Module):\n",
    "    def __init__(self, labels):\n",
    "        super(RobertaClassificationTwitter, self).__init__()\n",
    "        self.roberta = AutoModel.from_pretrained('cardiffnlp/twitter-roberta-base-sentiment-latest')\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        hidden_size = self.roberta.config.hidden_size\n",
    "\n",
    "        # Adding an additional hidden layer\n",
    "        self.hidden_layer = nn.Linear(hidden_size, hidden_size//2)\n",
    "        \n",
    "        # Adding L2 regularization (weight decay) to the hidden layer\n",
    "        self.regularization = nn.LayerNorm(hidden_size//2)\n",
    "        \n",
    "        # Final classification layer with 3 classes\n",
    "        self.classifier = nn.Linear(hidden_size//2, labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.roberta(input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs[1]\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        \n",
    "        # Passing through the hidden layer with ReLU activation\n",
    "        hidden_output = self.hidden_layer(pooled_output)\n",
    "        hidden_output = F.relu(hidden_output)\n",
    "        \n",
    "        # Applying Layer Normalization (regularization)\n",
    "        hidden_output = self.regularization(hidden_output)\n",
    "        \n",
    "        logits = self.classifier(hidden_output)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fn = FocalLoss(alpha=0.25, gamma=2)\n",
    "            loss = loss_fn(logits, labels)\n",
    "        \n",
    "        return {\"loss\": loss, \"logits\": logits} if loss is not None else {\"logits\": logits}\n",
    "    \n",
    "# Same like RobertaClassificationTwitter_3 but with distilbert\n",
    "class DistilbertClassification(nn.Module):\n",
    "    def __init__(self, labels):\n",
    "        super(DistilbertClassification, self).__init__()\n",
    "        self.distilbert = AutoModel.from_pretrained('distilbert-base-uncased')\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        hidden_size = self.distilbert.config.hidden_size\n",
    "\n",
    "        # Adding an additional hidden layer\n",
    "        self.hidden_layer = nn.Linear(hidden_size, hidden_size//2)\n",
    "        \n",
    "        # Adding L2 regularization (weight decay) to the hidden layer\n",
    "        self.regularization = nn.LayerNorm(hidden_size//2)\n",
    "        \n",
    "        # Final classification layer with 3 classes\n",
    "        self.classifier = nn.Linear(hidden_size//2, labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.distilbert(input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs[0][:, 0, :]\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        \n",
    "        # Passing through the hidden layer with ReLU activation\n",
    "        hidden_output = self.hidden_layer(pooled_output)\n",
    "        hidden_output = F.relu(hidden_output)\n",
    "        \n",
    "        # Applying Layer Normalization (regularization)\n",
    "        hidden_output = self.regularization(hidden_output)\n",
    "        \n",
    "        logits = self.classifier(hidden_output)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fn = FocalLoss(alpha=0.25, gamma=2)\n",
    "            loss = loss_fn(logits, labels)\n",
    "        \n",
    "        return {\"loss\": loss, \"logits\": logits} if loss is not None else {\"logits\": logits}\n",
    "\n",
    "class RobertaClassificationGoEmotions(nn.Module):\n",
    "    def __init__(self, labels):\n",
    "        super(RobertaClassificationGoEmotions, self).__init__()\n",
    "        self.roberta = AutoModel.from_pretrained('SamLowe/roberta-base-go_emotions')\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        hidden_size = self.roberta.config.hidden_size\n",
    "\n",
    "        # Adding an additional hidden layer\n",
    "        self.hidden_layer = nn.Linear(hidden_size, hidden_size//2)\n",
    "        \n",
    "        # Adding L2 regularization (weight decay) to the hidden layer\n",
    "        self.regularization = nn.LayerNorm(hidden_size//2)\n",
    "        \n",
    "        # Final classification layer with 3 classes\n",
    "        self.classifier = nn.Linear(hidden_size//2, labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.roberta(input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs[0][:, 0, :]\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        \n",
    "        # Passing through the hidden layer with ReLU activation\n",
    "        hidden_output = self.hidden_layer(pooled_output)\n",
    "        hidden_output = F.relu(hidden_output)\n",
    "        \n",
    "        # Applying Layer Normalization (regularization)\n",
    "        hidden_output = self.regularization(hidden_output)\n",
    "        \n",
    "        logits = self.classifier(hidden_output)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fn = FocalLoss(alpha=0.25, gamma=2)\n",
    "            loss = loss_fn(logits, labels)\n",
    "        \n",
    "        return {\"loss\": loss, \"logits\": logits} if loss is not None else {\"logits\": logits}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-12T15:39:52.437730Z",
     "iopub.status.busy": "2023-09-12T15:39:52.437265Z",
     "iopub.status.idle": "2023-09-12T15:39:53.471704Z",
     "shell.execute_reply": "2023-09-12T15:39:53.470932Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at SamLowe/roberta-base-go_emotions and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "labels = len(le.classes_)\n",
    "model = RobertaClassificationGoEmotions(labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-12T15:39:53.474360Z",
     "iopub.status.busy": "2023-09-12T15:39:53.474179Z",
     "iopub.status.idle": "2023-09-12T15:39:53.477830Z",
     "shell.execute_reply": "2023-09-12T15:39:53.477281Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = ClassificationDataset(train_encodings, train_labels)\n",
    "val_dataset = ClassificationDataset(val_encodings, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-12T15:39:53.480065Z",
     "iopub.status.busy": "2023-09-12T15:39:53.479619Z",
     "iopub.status.idle": "2023-09-12T15:39:54.600729Z",
     "shell.execute_reply": "2023-09-12T15:39:54.599845Z"
    }
   },
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-12T15:39:54.604090Z",
     "iopub.status.busy": "2023-09-12T15:39:54.603324Z",
     "iopub.status.idle": "2023-09-12T16:05:19.033188Z",
     "shell.execute_reply": "2023-09-12T16:05:19.032261Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1500' max='40000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 1500/40000 29:16 < 12:32:17, 0.85 it/s, Epoch 37/1000]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.372800</td>\n",
       "      <td>0.313802</td>\n",
       "      <td>0.527283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.283900</td>\n",
       "      <td>0.306421</td>\n",
       "      <td>0.578116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.233000</td>\n",
       "      <td>0.320606</td>\n",
       "      <td>0.585921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.333208</td>\n",
       "      <td>0.586581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.168700</td>\n",
       "      <td>0.350294</td>\n",
       "      <td>0.592778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.145000</td>\n",
       "      <td>0.379153</td>\n",
       "      <td>0.586471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.127300</td>\n",
       "      <td>0.405334</td>\n",
       "      <td>0.589913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.117200</td>\n",
       "      <td>0.425291</td>\n",
       "      <td>0.588822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.110000</td>\n",
       "      <td>0.438313</td>\n",
       "      <td>0.590596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.102600</td>\n",
       "      <td>0.446955</td>\n",
       "      <td>0.598399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.097800</td>\n",
       "      <td>0.487501</td>\n",
       "      <td>0.576587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.095800</td>\n",
       "      <td>0.477670</td>\n",
       "      <td>0.595697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.092200</td>\n",
       "      <td>0.484651</td>\n",
       "      <td>0.592935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>0.482133</td>\n",
       "      <td>0.588245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.086900</td>\n",
       "      <td>0.497652</td>\n",
       "      <td>0.574980</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.350293904542923, 'eval_f1_macro': 0.5927784645762696, 'eval_runtime': 2.2885, 'eval_samples_per_second': 554.079, 'eval_steps_per_second': 4.37, 'epoch': 37.5}\n"
     ]
    }
   ],
   "source": [
    "# Define training arguments and trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./output_text_pretrained',\n",
    "    per_device_train_batch_size=128,\n",
    "    per_device_eval_batch_size=128,\n",
    "    learning_rate=0.00001,\n",
    "    num_train_epochs=1000,\n",
    "    logging_dir='./logs',\n",
    "    evaluation_strategy='steps',\n",
    "    load_best_model_at_end=True,\n",
    "    logging_steps=100,\n",
    "    weight_decay=0.0001,\n",
    "    lr_scheduler_type='cosine',  # Using a cosine scheduler\n",
    "    warmup_steps=100  # Number of warmup steps\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=f1_macro,\n",
    "    callbacks=[ThresholdEarlyStoppingCallback(), StopOnZeroLossCallback(), StopOnF1MacroCallback()],\n",
    ")\n",
    "\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "eval_results = trainer.evaluate()\n",
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      change       0.59      0.49      0.54       332\n",
      "     neutral       0.76      0.81      0.79       786\n",
      "     sustain       0.45      0.46      0.45       150\n",
      "\n",
      "    accuracy                           0.69      1268\n",
      "   macro avg       0.60      0.59      0.59      1268\n",
      "weighted avg       0.68      0.69      0.68      1268\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Get predictions\n",
    "predictions, labels, _ = trainer.predict(val_dataset)\n",
    "predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(labels, predictions, target_names=le.classes_))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlStuff",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
